{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "swedish-lambda",
   "metadata": {},
   "source": [
    "작사가 인공지능 만들기\n",
    "========\n",
    "\n",
    "LSTM 모델과 셰익스피어 데이터셋을 사용해 간단한 작사가 인공지능을 만들어 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-ghost",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empty-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 라이브러리 불러오기\n",
    "import glob\n",
    "import os\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pursuant-special",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "# 파일에서 데이터 읽어오기\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/Exploration/E4/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])  # 앞에서부터 3라인만 화면에 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-cherry",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제\n",
    "\n",
    "  \n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하기    \n",
    "     \n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거    \n",
    "너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수 있기 때문에      \n",
    "문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exclusive-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i know you ' ll do the best you can . <end>\n"
     ]
    }
   ],
   "source": [
    "## 토큰화 하기 위한 전처리\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.'!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.'!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"I know you'll do the best you can.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-digest",
   "metadata": {},
   "source": [
    "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "4. a-zA-Z?.'!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "5. 다시 양쪽 공백을 지웁니다\n",
    "6. 문장 시작에는 \\<start>, 끝에는 \\<end>를 추가합니다     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "consolidated-english",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 171615\n",
      "Examples:\n",
      " ['<start> there must be some kind of way outta here <end>', '<start> said the joker to the thief <end>', \"<start> there ' s too much confusion <end>\", \"<start> i can ' t get no relief business men , they drink my wine <end>\", '<start> plowman dig my earth <end>', '<start> none were level on the mind <end>', '<start> nobody up at his word <end>', '<start> hey , hey no reason to get excited <end>', '<start> the thief he kindly spoke <end>', '<start> there are many here among us <end>']\n"
     ]
    }
   ],
   "source": [
    "## 정제 데이터 구축\n",
    "\n",
    "corpus = [] # 정제된 문장을 모음\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 길이가 0 이면 건너 뜀 \n",
    "    if len(sentence) == 0 : continue\n",
    "    #if len(sentence.split()) > 15 : continue\n",
    "        \n",
    "    # 정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    # 정제 문장이 이전 문장과 같으면 건너 뜀\n",
    "    if len(corpus) != 0 and preprocessed_sentence == corpus[-1]: continue\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "print(\"데이터 크기:\", len(corpus))\n",
    "print(\"Examples:\\n\", corpus[:10])  # 정제된 결과를 10개만 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-things",
   "metadata": {},
   "source": [
    "## 3. 평가 데이터셋 분리\n",
    "\n",
    "     \n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하기    \n",
    "단어장의 크기는 12,000 이상 으로 설정하기    \n",
    "총 데이터의 20% 를 평가 데이터셋으로 사용하기    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-academy",
   "metadata": {},
   "source": [
    "### tokenize() 함수로 데이터를 Tensor로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "embedded-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  65 279 ...   0   0   0]\n",
      " [  2 108   7 ...   0   0   0]\n",
      " [  2  65   4 ...   0   0   0]\n",
      " ...\n",
      " [  2  76  47 ... 872   3   0]\n",
      " [  2  50   5 ...   0   0   0]\n",
      " [  2  14 632 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9317317250>\n"
     ]
    }
   ],
   "source": [
    "## 토큰화 하기\n",
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 12000단어 이상을 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 14000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=14000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-sullivan",
   "metadata": {},
   "source": [
    "- tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 한 방에 해줍니다. 이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭합니다. 우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리되는 것입니다.\n",
    "\n",
    "\n",
    "- tf.keras.preprocessing.text.Tokenizer()    \n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "- tf.keras.preprocessing.sequence.pad_sequences()     \n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "arctic-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171615, 15)\n",
      "[[   2   65  279   28   99  524   20   85  790   94]\n",
      " [   2  108    7 6439   11    7 2507    3    0    0]\n",
      " [   2   65    4   17  103  181 2941    3    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)\n",
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incorrect-amplifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : '\n",
      "5 : ,\n",
      "6 : i\n",
      "7 : the\n",
      "8 : you\n",
      "9 : and\n",
      "10 : a\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전이 어떻게 구축되었는지 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-connecticut",
   "metadata": {},
   "source": [
    "### sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "innovative-edmonton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  65 279  28  99 524  20  85 790  94   3   0   0   0]\n",
      "[ 65 279  28  99 524  20  85 790  94   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "## 생성된 텐서를 소스와 타겟으로 분리하여 모델이 학습할 수 있게 하기\n",
    "\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confident-appeal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (137292, 14)\n",
      "Target Train: (137292, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-memory",
   "metadata": {},
   "source": [
    "- 데이터 정제 과정을 잘 거치면 학습 데이터 개수가 124960보다 작을 것이라고 하였습니다.      \n",
    "    하지만 데이터 정제를 제가 한 것 이외에 어떻게 더 해야할지 잘 모르겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-ethernet",
   "metadata": {},
   "source": [
    "## 4. 인공지능 만들기\n",
    "\n",
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "effective-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lyricist(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "lyricist = lyricist(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inappropriate-melbourne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4291/4291 [==============================] - 364s 83ms/step - loss: 3.6684\n",
      "Epoch 2/10\n",
      "4291/4291 [==============================] - 360s 84ms/step - loss: 2.9248\n",
      "Epoch 3/10\n",
      "4291/4291 [==============================] - 357s 83ms/step - loss: 2.6396\n",
      "Epoch 4/10\n",
      "4291/4291 [==============================] - 358s 84ms/step - loss: 2.3806\n",
      "Epoch 5/10\n",
      "4291/4291 [==============================] - 355s 83ms/step - loss: 2.1538\n",
      "Epoch 6/10\n",
      "4291/4291 [==============================] - 354s 82ms/step - loss: 1.9472\n",
      "Epoch 7/10\n",
      "4291/4291 [==============================] - 357s 83ms/step - loss: 1.7725\n",
      "Epoch 8/10\n",
      "4291/4291 [==============================] - 356s 83ms/step - loss: 1.6105\n",
      "Epoch 9/10\n",
      "4291/4291 [==============================] - 354s 83ms/step - loss: 1.4701\n",
      "Epoch 10/10\n",
      "4291/4291 [==============================] - 354s 83ms/step - loss: 1.3554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9310033b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-given",
   "metadata": {},
   "source": [
    "- loss 값은 최종 1.36으로 2.2 보다 낮은 것을 볼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-petersburg",
   "metadata": {},
   "source": [
    "### 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caroline-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyric(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "genetic-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love craig craig platter platter she she olivia olivia olivia womb womb womb territory territory gangster gangster bumped '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "increasing-allen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> hey you ooh ooh tripping tripping fronts sprayed desperado desperado affect booth booth rodeo kool kool humpty humpty humpty '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> hey you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "viral-purpose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my first homesick homesick gentleman gentleman gingerbread gingerbread gingerbread chicken learns learns learns learns learns learns boring boring boring '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> my first\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "helpful-relations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i want homesick homesick hottest hottest hottest whole whole '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> i want\", max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "offensive-blood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love craig craig platter platter she she olivia '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "senior-palace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> my love dion dion opening willing willing willing willing '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lyric(lyricist, tokenizer, init_sentence=\"<start> my love\", max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-dining",
   "metadata": {},
   "source": [
    "- 나온 가사들을 보면 중복 단어가 많은 것을 볼 수 있었다.    \n",
    "    그래도 노래 가사인 것을 생각하면 괜찮다고 생각한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
