{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "gross-emergency",
   "metadata": {},
   "source": [
    "프로젝트 : BERT Pretrained model의 활용\n",
    "==== \n",
    "\n",
    "-21.11.30-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-trainer",
   "metadata": {},
   "source": [
    "## pretrained model을 사용하지 않았을 때 BERT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signal-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detected-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디렉토리 지정\n",
    "data_dir = os.getenv('HOME')+'/aiffel/Exploration/E17/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/Exploration/E17/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-stroke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 한 데이터 로드 \n",
    "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sealed-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_seq_length': 384, 'max_query_length': 64}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "args = Config({\n",
    "    'max_seq_length': 384,\n",
    "    'max_query_length': 64,\n",
    "})\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lovely-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 데이터셋 파일을 메모리에 로딩하는 함수\n",
    "def load_data(args, filename):\n",
    "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
    "\n",
    "    n_discard = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
    "            data = json.loads(line)\n",
    "            # qa_id, title, question, context, answer, token_start, token_end\n",
    "            token_start = data.get(\"token_start\")\n",
    "            token_end = data.get(\"token_end\")\n",
    "            question = data[\"question\"][:args.max_query_length]\n",
    "            context = data[\"context\"]\n",
    "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
    "            context_len = args.max_seq_length - len(question) - 3\n",
    "\n",
    "            if token_end >= context_len:\n",
    "                # 최대 길이내에 token이 들어가지 않은 경우 처리하지 않음\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            context = context[:context_len]\n",
    "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
    "\n",
    "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
    "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "            ids += [0] * (args.max_seq_length - len(ids))\n",
    "            inputs.append(ids)\n",
    "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
    "            segs += [0] * (args.max_seq_length - len(segs))\n",
    "            segments.append(segs)\n",
    "            token_start += (len(question) + 2)\n",
    "            labels_start.append(token_start)\n",
    "            token_end += (len(question) + 2)\n",
    "            labels_end.append(token_end)\n",
    "    print(f'n_discard: {n_discard}')\n",
    "    \n",
    "    # question토큰+context토큰 , question(0)+context(1) / answer label start, end\n",
    "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-inflation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6444e284a4b4577b68a7f07920d8038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 430\n",
      "train_inputs: (59977, 384)\n",
      "train_inputs: (59977, 384)\n",
      "train_labels: (59977,)\n",
      "train_labels: (59977,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2387368da54995a9f90e4b75d13e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 78\n",
      "dev_inputs: (5696, 384)\n",
      "dev_inputs: (5696, 384)\n",
      "dev_labels: (5696,)\n",
      "dev_labels: (5696,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    19, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5, 21666,    19, ...,     0,     0,     0],\n",
       "         [    5,   964, 16865, ...,     0,     0,     0],\n",
       "         [    5,   365,    15, ...,     0,     0,     0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]])),\n",
       " (array([ 37, 184,  98, ...,  74, 190,  35]),\n",
       "  array([ 37, 185, 102, ...,  75, 191,  44])))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data load\n",
    "train_inputs, train_labels = load_data(args, train_json)\n",
    "print(f\"train_inputs: {train_inputs[0].shape}\")\n",
    "print(f\"train_inputs: {train_inputs[1].shape}\")\n",
    "print(f\"train_labels: {train_labels[0].shape}\")\n",
    "print(f\"train_labels: {train_labels[1].shape}\")\n",
    "\n",
    "# dev data load\n",
    "dev_inputs, dev_labels = load_data(args, dev_json)\n",
    "print(f\"dev_inputs: {dev_inputs[0].shape}\")\n",
    "print(f\"dev_inputs: {dev_inputs[1].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[0].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[1].shape}\")\n",
    "\n",
    "# data 확인\n",
    "train_inputs[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secret-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수들\n",
    "\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "    # gelu(x) = 0.5x * ( 1 + tanh( (2/pi)^1/2*(x + 0.44715x^3)) )\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "egyptian-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == \"embedding\" 일 경우 Token Embedding Layer 로 사용되는 layer 클래스입니다. \n",
    "\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shared Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "parental-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "attractive-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # Attention(Q, K, V) = softmax( QKT / √dk )V\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "canadian-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "square-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "essential-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "oriented-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_tokens, segments):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_tokens: encoder tokens\n",
    "        :param segments: token segments\n",
    "        :return logits_cls: CLS 결과 logits\n",
    "        :return logits_lm: LM 결과 logits\n",
    "        \"\"\"\n",
    "        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = enc_out\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "legal-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4KorQuAD(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(name='BERT4KorQuAD')\n",
    "\n",
    "        self.bert = BERT(config)\n",
    "        self.dense = tf.keras.layers.Dense(2)\n",
    "    \n",
    "    def call(self, enc_tokens, segments):\n",
    "        logits_cls, logits_lm = self.bert(enc_tokens, segments)\n",
    "\n",
    "        hidden = self.dense(logits_lm) # (bs, n_seq, 2)\n",
    "        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
    "\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
    "\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
    "\n",
    "        return start_outputs, end_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opposite-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 512,\n",
       " 'n_head': 8,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 6,\n",
       " 'n_seq': 384,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 512, \"n_head\": 8, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 6, \"n_seq\": 384, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "geological-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_batch_size = 32 \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "transparent-struggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT4KorQuAD(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pointed-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    p_bar = tqdm(dataset)\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "            start_loss = loss_fn(start_labels, start_outputs)\n",
    "            end_loss = loss_fn(end_labels, end_outputs)\n",
    "            loss = start_loss + end_loss\n",
    "\n",
    "            start_acc = acc_fn(start_labels, start_outputs)\n",
    "            end_acc = acc_fn(end_labels, end_outputs)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "        if batch % 10 == 9:\n",
    "            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')\n",
    "    p_bar.close()\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "neural-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):\n",
    "        start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "        start_loss = loss_fn(start_labels, start_outputs)\n",
    "        end_loss = loss_fn(end_labels, end_outputs)\n",
    "\n",
    "        start_acc = acc_fn(start_labels, start_outputs)\n",
    "        end_acc = acc_fn(end_labels, end_outputs)\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "frank-rubber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb40a2d48c9846749d13df1fa71b3234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 0 >> loss: 5.9506, 5.9506, acc: 0.0053, 0.0040\n",
      "save best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9823e4726468430babfc8207f28808e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 1 >> loss: 5.9506, 5.9506, acc: 0.0193, 0.0039\n",
      "save best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d388aa0706534103b2eaeb2715b8a468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 2 >> loss: 5.9506, 5.9506, acc: 0.0091, 0.0049\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e17e60f73349bfb7b89fa3b620e86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 3 >> loss: 5.9503, 5.9505, acc: 0.0074, 0.0053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704ee9f7fd6341888b7edc40fcd04496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 4 >> loss: 5.9506, 5.9506, acc: 0.0035, 0.0035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509d2f71ceca45a48590bd32689a53c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 5 >> loss: 5.9506, 5.9506, acc: 0.0004, 0.0016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac21763fbc94a4f9af0259c8b74f0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 6 >> loss: 5.9506, 5.9506, acc: 0.0049, 0.0007\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "best_acc = .0\n",
    "patience = 0\n",
    "for epoch in range(20):\n",
    "    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')\n",
    "    acc = start_acc + end_acc\n",
    "    if best_acc < acc:\n",
    "        patience = 0\n",
    "        best_acc = acc\n",
    "        model.save_weights(os.path.join(data_dir, \"korquad_bert_none_pretrain.hdf5\"))\n",
    "        print(f'save best model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 5 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-hostel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
